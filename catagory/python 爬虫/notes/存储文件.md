虽然在命令行里显示运行结果很有意思，但是随着数据不断增多，并且需要进行数据分析 时，将数据打印到命令行就不是办法了。为了可以远程使用大部分网络爬虫，你还需要把 采集到的数据存储起来。

本章将介绍三种主要的数据管理方法，对绝大多数应用都适用。如果你准备创建一个网站 的后端服务或者创建自己的 API，那么可能都需要让爬虫把数据写入数据库。如果你需要 一个快速简单的方法收集网上的文档，然后存到你的硬盘里，那么可能需要创建一个文件 流(file stream)来实现。如果还要为偶然事件提个醒儿，或者每天定时收集当天累计的数 据，就给自己发一封邮件吧!

抛开与网络数据采集的关系，大数据存储和与数据交互的能力，在新式的程序开发中也已 经是重中之重了。这一章的内容其实是实现第二部分许多示例的基础。如果你对自动数据 存储相关的知识不太了解，我非常希望你至少能浏览一下。

# 5.1 媒体文件

存储媒体文件有两种主要的方式:只获取文件 URL 链接，或者直接把源文件下载下来。
你可以通过媒体文件所在的 URL 链接直接引用它。这样做的优点如下:
   
      • 爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。
      • 可以节省很多存储空间，因为只需要存储 URL 链接就可以。
      • 存储 URL 的代码更容易写，也不需要实现文件下载代码。 • 不下载文件能够降低目标主机服务器的负载。
      
不过这么做也有一些缺点。
      
      • 这些内嵌在你的网站或应用中的外站 URL 链接被称为盗链(hotlinking)，使用盗链可 能会让你麻烦不断，每个网站都会实施防盗链措施。
      • 因为你的链接文件在别人的服务器上，所以你的应用就要跟着别人的节奏运行了。
      • 盗链是很容易改变的。如果你把盗链图片放在博客上，要是被对方服务器发现，很可能 被恶搞。如果你把 URL 链接存起来准备以后再用，可能用的时候链接已经失效了，或
       者是变成了完全无关的内容。
      • 现实中的网络浏览器不仅可以请求 HTML 页面并切换页面，它们也会下载访问页面上
       所有的资源。下载文件会让你的爬虫看起来更像是人在浏览网站，这样做反而有好处。

如果你还在犹豫究竟是存储文件，还是只存储文件的 URL 链接，可以想想这些文件是要 多次使用，还是放进数据库之后就只是等着“落灰”，再也不会被打开。如果答案是后者， 那么最好还是只存储这些文件的 URL 吧。如果答案是前者，那么就继续往下看!

在 Python 3.x 版本中，urllib.request.urlretrieve 可以根据文件的 URL 下载文件:

    from urllib.request import urlretrieve 
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup
    
    html = urlopen("http://www.pythonscraping.com")
    bsObj = BeautifulSoup(html)
    imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
    urlretrieve (imageLocation, "logo.jpg")
    
  这段程序从 http://pythonscraping.com 下载 logo 图片，然后在程序运行的文件夹里保存为 logo.jpg 文件。
  
如果你只需要下载一个文件，而且知道如何获取它，以及它的文件类型，这么做就可以 了。但是大多数爬虫都不可能一天只下载一个文件。下面的程序会把 http://pythonscraping. com 主页上所有 src 属性的文件都下载下来:

               import os
               from urllib.request import urlretrieve 
               from urllib.request import urlopen 
               from bs4 import BeautifulSoup

               downloadDirectory = "downloaded"
               baseUrl = "http://pythonscraping.com"

               def getAbsoluteURL(baseUrl, source):
                     if source.startswith("http://www."):
                           url = "http://"+source[11:]
                     elif source.startswith("http://"):
                           url = source
                     elif source.startswith("www."):
                           url = source[4:]
                           url = "http://"+source
                     else:
                           url = baseUrl+"/"+source 
                     if baseUrl not in url:
                           return None 
                  return url

               def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):
                     path = absoluteUrl.replace("www.", "")
                     path = path.replace(baseUrl, "")
                     path = downloadDirectory+path
                     directory = os.path.dirname(path)
                     if not os.path.exists(directory): 
                        os.makedirs(directory)
                     return path

                html = urlopen("http://www.pythonscraping.com")
                bsObj = BeautifulSoup(html)
                downloadList = bsObj.findAll(src=True)

                for download in downloadList:
                     fileUrl = getAbsoluteURL(baseUrl, download["src"]) 
                     if fileUrl is not None:
                        print(fileUrl)



               urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))

         程序运行注意事项
         你知道从网上下载未知文件的那些警告吗?这个程序会把页面上所有的文件 下载到你的硬盘里，可能会包含一些 bash 脚本、.exe 文件，甚至可能是恶意 软件(malware)。
         如果你之前从没有运行过任何下载到电脑里的文件，电脑就是安全的吗?尤 其是当你用管理员权限运行这个程序时，你的电脑基本已经处于危险之中。 如果你执行了网页上的一个文件，那个文件把自己传送到了 ../../../../usr/bin/ python 里面，会发生什么呢?等下一次你再运行 Python 程序时，你的电脑 就可能会安装恶意软件。
         这个程序只是为了演示;请不要随意运行它，因为这里没有对所有下载文件 的类型进行检查，也不应该用管理员权限运行它。记得经常备份重要的文 件，不要在硬盘上存储敏感信息，小心驶得万年船。
         
这个程序首先使用 Lambda 函数(第 2 章介绍过)选择首页上所有带 src 属性的标签。然 后对 URL 链接进行清理和标准化，获得文件的绝对路径(而且去掉了外链)。最后，每个文件都会下载到程序所在文件夹的 downloaded 文件里。

这里 Python 的 os 模块用来获取每个下载文件的目标文件夹，建立完整的路径。os 模块是 Python 与操作系统进行交互的接口，它可以操作文件路径，创建目录，获取运行进程和环 境变量的信息，以及其他系统相关的操作。


# 5.2 把数据存储到CSV

CSV(Comma-Separated Values，逗号分隔值)是存储表格数据的常用文件格式。Microsoft
Excel 和很多应用都支持 CSV 格式，因为它很简洁。下面就是一个 CSV 文件的例子

     fruit,cost
     apple,1.00
     banana,0.30
     pear,1.25
     
和 Python 一样，CSV 里留白(whitespace)也是很重要的:每一行都用一个换行符分隔， 列与列之间用逗号分隔(因此也叫“逗号分隔值”)。CSV 文件还可以用 Tab 字符或其他字 符分隔行，但是不太常见，用得不多。

如果你只想从网页上把 CSV 文件下载到电脑里，不打算做任何解析和修改，那么这节后 面的内容就没必要再看了。只要用上一节里介绍的文件下载方法下载并保存为 CSV 格式 就行了。

Python 的 csv 库可以非常简单地修改 CSV 文件，甚至从零开始创建一个 CSV 文件:

         import csv
         csvFile = open("../files/test.csv", 'w+') 
         try:
            writer = csv.writer(csvFile)
            writer.writerow(('number', 'number plus 2', 'number times 2')) 
            for i in range(10):
               writer.writerow( (i, i+2, i*2)) 
         finally:
                  csvFile.close()

这里提个醒儿:Python 新建文件的机制考虑得非常周到(bullet-proof)。如果 ../files/test.csv 不存在，Python 会自动创建文件(不会自动创建文件夹)。如果文件已经存在，Python 会 用新的数据覆盖 test.csv 文件。


运行完成后，你会看到一个 CSV 文件:

      number,number plus 2,number times 2
      0,2,0
      1,3,2
      2,4,4 ...

网络数据采集的一个常用功能就是获取 HTML 表格并写入 CSV 文件。维基百科的文本编 辑 器 对 比 词 条(https://en.wikipedia.org/wiki/Comparison_of_text_editors) 中 用 了 许 多 复 杂 的 HTML 表格，用到了颜色、链接、排序，以及其他在写入 CSV 文件之前需要忽略的 HTML 元素。用 BeautifulSoup 和 get_text() 函数，你可以用十几行代码完成这件事:


