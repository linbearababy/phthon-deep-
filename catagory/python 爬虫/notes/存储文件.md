虽然在命令行里显示运行结果很有意思，但是随着数据不断增多，并且需要进行数据分析 时，将数据打印到命令行就不是办法了。为了可以远程使用大部分网络爬虫，你还需要把 采集到的数据存储起来。

本章将介绍三种主要的数据管理方法，对绝大多数应用都适用。如果你准备创建一个网站 的后端服务或者创建自己的 API，那么可能都需要让爬虫把数据写入数据库。如果你需要 一个快速简单的方法收集网上的文档，然后存到你的硬盘里，那么可能需要创建一个文件 流(file stream)来实现。如果还要为偶然事件提个醒儿，或者每天定时收集当天累计的数 据，就给自己发一封邮件吧!

抛开与网络数据采集的关系，大数据存储和与数据交互的能力，在新式的程序开发中也已 经是重中之重了。这一章的内容其实是实现第二部分许多示例的基础。如果你对自动数据 存储相关的知识不太了解，我非常希望你至少能浏览一下。

# 5.1 媒体文件

存储媒体文件有两种主要的方式:只获取文件 URL 链接，或者直接把源文件下载下来。
你可以通过媒体文件所在的 URL 链接直接引用它。这样做的优点如下:
   
      • 爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。
      • 可以节省很多存储空间，因为只需要存储 URL 链接就可以。
      • 存储 URL 的代码更容易写，也不需要实现文件下载代码。 • 不下载文件能够降低目标主机服务器的负载。
      
不过这么做也有一些缺点。
      
      • 这些内嵌在你的网站或应用中的外站 URL 链接被称为盗链(hotlinking)，使用盗链可 能会让你麻烦不断，每个网站都会实施防盗链措施。
      • 因为你的链接文件在别人的服务器上，所以你的应用就要跟着别人的节奏运行了。
      • 盗链是很容易改变的。如果你把盗链图片放在博客上，要是被对方服务器发现，很可能 被恶搞。如果你把 URL 链接存起来准备以后再用，可能用的时候链接已经失效了，或
       者是变成了完全无关的内容。
      • 现实中的网络浏览器不仅可以请求 HTML 页面并切换页面，它们也会下载访问页面上
       所有的资源。下载文件会让你的爬虫看起来更像是人在浏览网站，这样做反而有好处。

如果你还在犹豫究竟是存储文件，还是只存储文件的 URL 链接，可以想想这些文件是要 多次使用，还是放进数据库之后就只是等着“落灰”，再也不会被打开。如果答案是后者， 那么最好还是只存储这些文件的 URL 吧。如果答案是前者，那么就继续往下看!

在 Python 3.x 版本中，urllib.request.urlretrieve 可以根据文件的 URL 下载文件:

    from urllib.request import urlretrieve 
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup
    
    html = urlopen("http://www.pythonscraping.com")
    bsObj = BeautifulSoup(html)
    imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
    urlretrieve (imageLocation, "logo.jpg")
    
  这段程序从 http://pythonscraping.com 下载 logo 图片，然后在程序运行的文件夹里保存为 logo.jpg 文件。
  
如果你只需要下载一个文件，而且知道如何获取它，以及它的文件类型，这么做就可以 了。但是大多数爬虫都不可能一天只下载一个文件。下面的程序会把 http://pythonscraping. com 主页上所有 src 属性的文件都下载下来:

               import os
               from urllib.request import urlretrieve 
               from urllib.request import urlopen 
               from bs4 import BeautifulSoup

               downloadDirectory = "downloaded"
               baseUrl = "http://pythonscraping.com"

               def getAbsoluteURL(baseUrl, source):
                     if source.startswith("http://www."):
                           url = "http://"+source[11:]
                     elif source.startswith("http://"):
                           url = source
                     elif source.startswith("www."):
                           url = source[4:]
                           url = "http://"+source
                     else:
                           url = baseUrl+"/"+source 
                     if baseUrl not in url:
                           return None 
                  return url

               def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):
                     path = absoluteUrl.replace("www.", "")
                     path = path.replace(baseUrl, "")
                     path = downloadDirectory+path
                     directory = os.path.dirname(path)
                     if not os.path.exists(directory): 
                        os.makedirs(directory)
                     return path

                html = urlopen("http://www.pythonscraping.com")
                bsObj = BeautifulSoup(html)
                downloadList = bsObj.findAll(src=True)

                for download in downloadList:
                     fileUrl = getAbsoluteURL(baseUrl, download["src"]) 
                     if fileUrl is not None:
                        print(fileUrl)



               urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))

         程序运行注意事项
         你知道从网上下载未知文件的那些警告吗?这个程序会把页面上所有的文件 下载到你的硬盘里，可能会包含一些 bash 脚本、.exe 文件，甚至可能是恶意 软件(malware)。
         如果你之前从没有运行过任何下载到电脑里的文件，电脑就是安全的吗?尤 其是当你用管理员权限运行这个程序时，你的电脑基本已经处于危险之中。 如果你执行了网页上的一个文件，那个文件把自己传送到了 ../../../../usr/bin/ python 里面，会发生什么呢?等下一次你再运行 Python 程序时，你的电脑 就可能会安装恶意软件。
         这个程序只是为了演示;请不要随意运行它，因为这里没有对所有下载文件 的类型进行检查，也不应该用管理员权限运行它。记得经常备份重要的文 件，不要在硬盘上存储敏感信息，小心驶得万年船。
         
这个程序首先使用 Lambda 函数(第 2 章介绍过)选择首页上所有带 src 属性的标签。然 后对 URL 链接进行清理和标准化，获得文件的绝对路径(而且去掉了外链)。最后，每个文件都会下载到程序所在文件夹的 downloaded 文件里。

这里 Python 的 os 模块用来获取每个下载文件的目标文件夹，建立完整的路径。os 模块是 Python 与操作系统进行交互的接口，它可以操作文件路径，创建目录，获取运行进程和环 境变量的信息，以及其他系统相关的操作。


# 5.2 把数据存储到CSV

CSV(Comma-Separated Values，逗号分隔值)是存储表格数据的常用文件格式。Microsoft
Excel 和很多应用都支持 CSV 格式，因为它很简洁。下面就是一个 CSV 文件的例子

     fruit,cost
     apple,1.00
     banana,0.30
     pear,1.25
     
和 Python 一样，CSV 里留白(whitespace)也是很重要的:每一行都用一个换行符分隔， 列与列之间用逗号分隔(因此也叫“逗号分隔值”)。CSV 文件还可以用 Tab 字符或其他字 符分隔行，但是不太常见，用得不多。

如果你只想从网页上把 CSV 文件下载到电脑里，不打算做任何解析和修改，那么这节后 面的内容就没必要再看了。只要用上一节里介绍的文件下载方法下载并保存为 CSV 格式 就行了。

Python 的 csv 库可以非常简单地修改 CSV 文件，甚至从零开始创建一个 CSV 文件:

         import csv
         csvFile = open("../files/test.csv", 'w+') 
         try:
            writer = csv.writer(csvFile)
            writer.writerow(('number', 'number plus 2', 'number times 2')) 
            for i in range(10):
               writer.writerow( (i, i+2, i*2)) 
         finally:
                  csvFile.close()

这里提个醒儿:Python 新建文件的机制考虑得非常周到(bullet-proof)。如果 ../files/test.csv 不存在，Python 会自动创建文件(不会自动创建文件夹)。如果文件已经存在，Python 会 用新的数据覆盖 test.csv 文件。


运行完成后，你会看到一个 CSV 文件:

      number,number plus 2,number times 2
      0,2,0
      1,3,2
      2,4,4 ...

网络数据采集的一个常用功能就是获取 HTML 表格并写入 CSV 文件。维基百科的文本编 辑 器 对 比 词 条(https://en.wikipedia.org/wiki/Comparison_of_text_editors) 中 用 了 许 多 复 杂 的 HTML 表格，用到了颜色、链接、排序，以及其他在写入 CSV 文件之前需要忽略的 HTML 元素。用 BeautifulSoup 和 get_text() 函数，你可以用十几行代码完成这件事:

         import csv
         from urllib.request import urlopen 
         from bs4 import BeautifulSoup
         html = urlopen("http://en.wikipedia.org/wiki/Comparison_of_text_editors") 
         bsObj = BeautifulSoup(html)
         # 主对比表格是当前页面上的第一个表格
         table = bsObj.findAll("table",{"class":"wikitable"})[0]
         rows = table.findAll("tr")
         
         csvFile = open("../files/editors.csv", 'wt', newline=", encoding='utf-8')
         writer = csv.writer(csvFile)
         try:
            for row in rows:
            csvRow = []
            for cell in row.findAll(['td', 'th']):
                     csvRow.append(cell.get_text())
                     writer.writerow(csvRow) 
         finally:
            csvFile.close()


         实际工作中写此程序之前的注意事项
      如果你有很多 HTML 表格，且每个都要转换成 CSV 文件，或者许多 HTML 表格都要 汇总到一个 CSV 文件，那么把这个程序整合到爬虫里以解决问题非常好。但是，如果 你只需要做一次这种事情，那么更好的办法就是:复制粘贴。选择 HTML 表格内容然 后粘贴到 Excel 文件里，可以另存为 CSV 格式，不需要写代码就能搞定!
      
这个程序会在程序上一层目录的 files 文件夹里生成一个 CSV 文件 ../files/editors.csv——把 这个程序分享给那些不熟悉 MySQL 的朋友们吧!

# 5.3 MySQL
MySQL(官方发音是“My es-kew-el”，但很多人都说成“My Sequel”)是目前最受欢迎 的开源关系型数据库管理系统。一个开源项目具有如此之竞争力实在是令人意外，它的流 行程度正在不断地接近另外两个闭源的商业数据库系统:微软的 SQL Server 和甲骨文的 Oracle 数据库(MySQL 在 2010 年被甲骨文收购)。

它的流程程度实在是名符其实。对大多数应用来说，MySQL 都是不二选择。它是一种 非常灵活、稳定、功能齐全的 DBMS，许多顶级的网站都在用它:YouTube1、Twitter2 和 Facebook3 等


因为它受众广泛，免费，开箱即用，所以它也是网络数据采集项目中常用的数据库，我们 将在本书后面的示例中使用它。

         “关系型”数据库 ? “关系型数据”就是有关联的数据。就是这么简单!
         开个玩笑!当计算机科学家说起关系型数据时，他们指的是那些并非孤立的数据—— 它们的属性与其他的数据是有关联的。例如，“用户 A 在学校 B 上学”，这里用户 A 在数据库的“用户”表中，而学校 B 是在数据库的“学校”表中。
         在本章后面的内容里，我们将介绍数据关系的不同类型，以及如何有效地把数据存储 到 MySQL(或其他关系型数据库)里。
         
# 5.3.1 安装MySQL
如果你第一次接触 MySQL，安装数据库听着可能有点儿吓人(如果你是老手，可以跳过 这部分内容)。其实，安装方法和安装其他软件一样简单。归根到底，MySQL 就是由一系 列数据文件构成的，储存在你的远端服务器或本地的电脑上，里面包含了数据库存储的所 有信息。MySQL 软件层提供了一种与数据交互的便捷操作方法。例如，下面的命令把用 户表 users 中名字为“Ryan”的用户找出来:         

      SELECT * FROM users WHERE firstname = "Ryan"
      
如果你用 Ubuntu(或其他 Debain 分支系统)，安装 MySQL 很简单:

       $sudo apt-get install mysql-server
       

只要稍微留意一下安装过程，看看电脑是不是可以满足安装的内存需求，然后在安装提示 的地方为 root 用户设置新密码就可以了


Mac OS X 和 Windows 系统的安装过程有点儿复杂。如果你没有甲骨文账户，下载 MySQL 安装包之前需要先注册一下。

如 果 你 用 Mac OS X 系 统， 请 先 下 载 对 应 的 安 装 包(http://dev.mysql.com/downloads/ mysql/)。

选择 .dmg 安装包，登入网站或者创建一个账户，开始下载文件。下载完成后打开安装包， 你会看到一个简单的安装向导(图 5-1)。

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-09%2022.52.01.png)

使用默认安装步骤就可以，本书后面使用 MySQL 都是假设你用的默认安装步骤。

如果觉得下载安装包再执行安装工具太无聊，也可以用 Mac OS X 的包管理器 Homebrew (http://brew.sh/)安装。当 Homebrew 安装好以后，用下面的命令安装 MySQL:

       $brew install mysql
       
Homebrew 是一个伟大的开源工具，与 Python 包完美结合。其实，本书使用的大多数 Python 第三方库都可以用 Homebrew 安装。如果你还没用过，强烈推荐你试一下!

Mac OS X 的 MySQL 安装好之后，你可以用下面的命令启动 MySQL 服务器: 

        $cd /usr/local/mysql
        $sudo ./bin/mysqld_safe
        
在 Windows 系统上，安装和运行 MySQL 更复杂一些，但是有个方便的安装工具(http:// dev.mysql.com/downloads/windows/installer/)可以简化这个过程(图 5-2)。

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-09%2022.52.01.png)

用默认选项安装 MySQL 就可以，不过有一个地方要注意:在 Setup Type(类型设置)页 面，建议你选择“Server Only”(只选服务器)选项，这可以避免安装一堆微软的软件和库 文件。
然后，你就可以用默认设置安装，跟着提示一步步操作就可以启动 MySQL 服务器了。

# 5.3.2 基本命令
MySQL 服务器启动之后，有很多种方法可以与数据库交互。因为有很多工具是图形界面， 所以你不用 MySQL 的命令行(或者很少用命令行)也能管理数据库。像 phpMyAdmin 和 MySQL Workbench 这类工具都可以很容易地实现数据的查看、排序和新建等工作。但是， 掌握命令行操作数据库依然是很重要的。

除了用户自定义变量名(MySQL 5.x 版本是不区分大小写的，MySQL 5.0 之前的版本是不 区分大小写的)，MySQL 语句是不区分大小写的。例如，SELECT 和 sElEcT 是一样的，不 过习惯上写 MySQL 语句的时候所有的 MySQL 关键词都用大写。大多数开发者还喜欢用 小写字母表示数据表和数据库的名称，虽然这个标准经常不被注意。

当你首次登入 MySQL 的时候，里面是没有数据库存放数据的。你可以创建一个:

      >CREATE DATABASE scraping;
   
 因为每个 MySQL 实例可以有多个数据库，所以使用某个数据库之前需要指定数据库的 名称:
 
      >USE scraping;
      
从现在开始(直到关闭 MySQL 链接或切换到另一个数据库之前)，所有的命令都运行在这
个新的“scraping”数据库里面.

 所有操作看着都非常简单。那么，在数据库里创建数据表的方法应该也类似吧?让我们在
 数据库里创建一个表来存储采集的网页:

      >CREATE TABLE pages;
      
结果显示错误:

       ERROR 1113 (42000): A table must have at least 1 column
       
和数据库不同，MySQL 数据表必须至少有一列，否则不能创建。为了在 MySQL 里定义字 段(数据列)，你必须在 CREATE TABLE <tablename> 语句后面，把字段的定义放进一个带括 号的、内部由逗号分隔的列表中:
   
      >CREATE TABLE pages (id BIGINT(7) NOT NULL AUTO_INCREMENT, title VARCHAR(200), content VARCHAR(10000), created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id));

每个字段定义由三部分组成:

      
      • 名称(id、title、created 等)
      • 数据类型(BIGINT(7)、VARCHAR、TIMESTAMP) 
      • 其他可选属性(NOT NULL AUTO_INCREMENT)
      
在字段定义列表的最后，还要定义一个“主键”(key)。MySQL 用这个主键来组织表的内 容，便于后面快速查询。在本章后面的内容里，我将介绍如何调整这些主键以提高数据库 的查询速度，但是现在，用表的 id 列作为主键就可以。

语句执行之后，你可以用 DESCRIBE 查看数据表的结构:


    > DESCRIBE pages;
          +---------+----------------+------+-----+-------------------+----------------+
          | Field   | Type           | Null | Key | Default           | Extra          |
          +---------+----------------+------+-----+-------------------+----------------+
          | id      | bigint(7)      | NO   | PRI | NULL              | auto_increment |
          | title   | varchar(200)   | YES  |     | NULL              |                |
          | content | varchar(10000) | YES  |     | NULL              |                |
          | created | timestamp      | NO   |     | CURRENT_TIMESTAMP |                |
         +---------+----------------+------+-----+-------------------+----------------+
         4 rows in set (0.00 sec)

当然，这还是一个空表。你可以在 pages 表里插入一些测试数据，如下所示:

      > INSERT INTO pages (title, content) VALUES ("Test page title", "This is some te
           st page content. It can be up to 10,000 characters long.");
     
需要注意的是，虽然 pages 表里有四个字段(id、title、content、created)，但实际上 你只需要插入两个字段(title 和 content)的数据即可。因为 id 字段是自动递增的(每 次新插入数据行时 MySQL 自动增加 1)，通常不用处理。另外，created 字段的类型是 timestamp，默认就是数据加入时的时间戳。

当然，我们也可以自定义四个字段的内容:

      INSERT INTO pages (id, title, content, created) VALUES (3, "Test page title", " This is some test page content. It can be up to 10,000 characters long.", "2014- 09-21 10:25:32");
      
只要你定义的整数在数据表的 id 字段里没有，它就可以顺利插入数据表。但是，这么做 非常不好;除非万不得已(比如程序中断漏了一行数据)，否则让 MySQL 自己处理 id 和 timestamp 字段。

现在表里有一些数据了，你可以用很多方法来选择这些数据。下面是几个 SELECT 语句的 示例:

      >SELECT * FROM pages WHERE id = 2;
      
这条语句告诉 MySQL，“从 pages 表中把 id 字段中等于 2 的整行数据全挑选出来”。这个 星号(*)是通配符，表示所有字段，这行语句会把满足条件(WHERE id = 2)的所有字段 都显示出来。如果 id 字段里没有任何一行等于 2，就会返回一个空集。例如，下面这个不 区分大小写的查询，会返回 title 字段里包含“test”的所有行(% 符合表示 MySQL 字符 串通配符)的所有字段:

      >SELECT * FROM pages WHERE title LIKE "%test%";
      
但是，如果你的表有很多字段，而你只想返回部分字段怎么办?你可以不用星号，而用下
面的方式:

      >SELECT id, title FROM pages WHERE content LIKE "%page content%"; 
      
这样就只会返回 title 字段包含“page content”的所有行的 id 和 title 两个字段了。 DELETE 语句语法与 SELECT 语句类似:

      >DELETE FROM pages WHERE id = 1;
      
由于数据库的数据删除后不能恢复，所以在执行 DELETE 语句之前，建议用 SELECT 确认 一下要删除的数据(本例中，就是用 SELECT * FROM pages WHERE id = 1 查看)，然后把 SELECT * 换成 DELETE 就可以了，这会是一个好习惯。很多程序员都有过一些 DELETE 误操 作的伤心往事，还有一些恐怖故事就是有人慌乱中忘了在语句中放 WHERE，结果把所有客 户数据都删除了。别让这种事发生在你身上!
还有一个需要介绍的语句是 UPDATE:

      >UPDATE pages SET title="A new title", content="Some new content" WHERE id=2;
      
结合本书的主题，后面我们就只用这些基本的 MySQL 语句，做一些简单的数据查询、创 建和更新工作。如果你对这个强大数据库的命令和技术感兴趣，推荐你去看 Paul DuBois 的 MySQL Cookbook(http://shop.oreilly.com/product/0636920032274.do)

# summay: 
我按书上的执行操作会有误差（我的电脑是Mac） 可能是因为机器更新和mysql是8.0最新版的原因： 

我用终端从homedrew下载 mysql 之后不会执行，打开不开mysql.

所以我从MySQL Community Server下载了安装包： 

打开MySQL Community Server网页， 选择 .DMG 安装包；

注册账户，需要邮箱；

下载安装包；

之后默认安装包选项，直到下载完；

用终端打开mysql（执行如下命令）：

PATH="$PATH":/usr/local/mysql/bin

然后执行

mysql -u root -p

之后如果有密码就输入密码，没有密码就按enter建。

然后mysql就打开了，就可以运行了。

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2015.16.13.png)

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2015.16.27.png)

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2015.16.41.png)

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2015.16.50.png)

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2015.16.58.png)



![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%2013.53.45.png)


