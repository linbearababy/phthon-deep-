虽然在命令行里显示运行结果很有意思，但是随着数据不断增多，并且需要进行数据分析 时，将数据打印到命令行就不是办法了。为了可以远程使用大部分网络爬虫，你还需要把 采集到的数据存储起来。

本章将介绍三种主要的数据管理方法，对绝大多数应用都适用。如果你准备创建一个网站 的后端服务或者创建自己的 API，那么可能都需要让爬虫把数据写入数据库。如果你需要 一个快速简单的方法收集网上的文档，然后存到你的硬盘里，那么可能需要创建一个文件 流(file stream)来实现。如果还要为偶然事件提个醒儿，或者每天定时收集当天累计的数 据，就给自己发一封邮件吧!

抛开与网络数据采集的关系，大数据存储和与数据交互的能力，在新式的程序开发中也已 经是重中之重了。这一章的内容其实是实现第二部分许多示例的基础。如果你对自动数据 存储相关的知识不太了解，我非常希望你至少能浏览一下。

# 5.1 媒体文件

存储媒体文件有两种主要的方式:只获取文件 URL 链接，或者直接把源文件下载下来。
你可以通过媒体文件所在的 URL 链接直接引用它。这样做的优点如下:
   
      • 爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。
      • 可以节省很多存储空间，因为只需要存储 URL 链接就可以。
      • 存储 URL 的代码更容易写，也不需要实现文件下载代码。 • 不下载文件能够降低目标主机服务器的负载。
      
不过这么做也有一些缺点。
      
      • 这些内嵌在你的网站或应用中的外站 URL 链接被称为盗链(hotlinking)，使用盗链可 能会让你麻烦不断，每个网站都会实施防盗链措施。
      • 因为你的链接文件在别人的服务器上，所以你的应用就要跟着别人的节奏运行了。
      • 盗链是很容易改变的。如果你把盗链图片放在博客上，要是被对方服务器发现，很可能 被恶搞。如果你把 URL 链接存起来准备以后再用，可能用的时候链接已经失效了，或
       者是变成了完全无关的内容。
      • 现实中的网络浏览器不仅可以请求 HTML 页面并切换页面，它们也会下载访问页面上
       所有的资源。下载文件会让你的爬虫看起来更像是人在浏览网站，这样做反而有好处。

如果你还在犹豫究竟是存储文件，还是只存储文件的 URL 链接，可以想想这些文件是要 多次使用，还是放进数据库之后就只是等着“落灰”，再也不会被打开。如果答案是后者， 那么最好还是只存储这些文件的 URL 吧。如果答案是前者，那么就继续往下看!

在 Python 3.x 版本中，urllib.request.urlretrieve 可以根据文件的 URL 下载文件:

    from urllib.request import urlretrieve 
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup
    
    html = urlopen("http://www.pythonscraping.com")
    bsObj = BeautifulSoup(html)
    imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
    urlretrieve (imageLocation, "logo.jpg")
    
  这段程序从 http://pythonscraping.com 下载 logo 图片，然后在程序运行的文件夹里保存为 logo.jpg 文件。
  
如果你只需要下载一个文件，而且知道如何获取它，以及它的文件类型，这么做就可以 了。但是大多数爬虫都不可能一天只下载一个文件。下面的程序会把 http://pythonscraping. com 主页上所有 src 属性的文件都下载下来:

               import os
               from urllib.request import urlretrieve 
               from urllib.request import urlopen 
               from bs4 import BeautifulSoup

               downloadDirectory = "downloaded"
               baseUrl = "http://pythonscraping.com"

               def getAbsoluteURL(baseUrl, source):
                     if source.startswith("http://www."):
                           url = "http://"+source[11:]
                     elif source.startswith("http://"):
                           url = source
                     elif source.startswith("www."):
                           url = source[4:]
                           url = "http://"+source
                     else:
                           url = baseUrl+"/"+source 
                     if baseUrl not in url:
                           return None 
                  return url

               def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):
                     path = absoluteUrl.replace("www.", "")
                     path = path.replace(baseUrl, "")
                     path = downloadDirectory+path
                     directory = os.path.dirname(path)
                     if not os.path.exists(directory): 
                        os.makedirs(directory)
                     return path

                html = urlopen("http://www.pythonscraping.com")
                bsObj = BeautifulSoup(html)
                downloadList = bsObj.findAll(src=True)

                for download in downloadList:
                     fileUrl = getAbsoluteURL(baseUrl, download["src"]) 
                     if fileUrl is not None:
                        print(fileUrl)



               urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))

         程序运行注意事项
         你知道从网上下载未知文件的那些警告吗?这个程序会把页面上所有的文件 下载到你的硬盘里，可能会包含一些 bash 脚本、.exe 文件，甚至可能是恶意 软件(malware)。
         如果你之前从没有运行过任何下载到电脑里的文件，电脑就是安全的吗?尤 其是当你用管理员权限运行这个程序时，你的电脑基本已经处于危险之中。 如果你执行了网页上的一个文件，那个文件把自己传送到了 ../../../../usr/bin/ python 里面，会发生什么呢?等下一次你再运行 Python 程序时，你的电脑 就可能会安装恶意软件。
         这个程序只是为了演示;请不要随意运行它，因为这里没有对所有下载文件 的类型进行检查，也不应该用管理员权限运行它。记得经常备份重要的文 件，不要在硬盘上存储敏感信息，小心驶得万年船。
         
这个程序首先使用 Lambda 函数(第 2 章介绍过)选择首页上所有带 src 属性的标签。然 后对 URL 链接进行清理和标准化，获得文件的绝对路径(而且去掉了外链)。最后，每个文件都会下载到程序所在文件夹的 downloaded 文件里。

这里 Python 的 os 模块用来获取每个下载文件的目标文件夹，建立完整的路径。os 模块是 Python 与操作系统进行交互的接口，它可以操作文件路径，创建目录，获取运行进程和环 境变量的信息，以及其他系统相关的操作。


# 5.2 把数据存储到CSV

CSV(Comma-Separated Values，逗号分隔值)是存储表格数据的常用文件格式。Microsoft
Excel 和很多应用都支持 CSV 格式，因为它很简洁。下面就是一个 CSV 文件的例子

     fruit,cost
     apple,1.00
     banana,0.30
     pear,1.25
     
和 Python 一样，CSV 里留白(whitespace)也是很重要的:每一行都用一个换行符分隔， 列与列之间用逗号分隔(因此也叫“逗号分隔值”)。CSV 文件还可以用 Tab 字符或其他字 符分隔行，但是不太常见，用得不多。

如果你只想从网页上把 CSV 文件下载到电脑里，不打算做任何解析和修改，那么这节后 面的内容就没必要再看了。只要用上一节里介绍的文件下载方法下载并保存为 CSV 格式 就行了。

Python 的 csv 库可以非常简单地修改 CSV 文件，甚至从零开始创建一个 CSV 文件:

         import csv
         csvFile = open("../files/test.csv", 'w+') 
         try:
            writer = csv.writer(csvFile)
            writer.writerow(('number', 'number plus 2', 'number times 2')) 
            for i in range(10):
               writer.writerow( (i, i+2, i*2)) 
         finally:
                  csvFile.close()

这里提个醒儿:Python 新建文件的机制考虑得非常周到(bullet-proof)。如果 ../files/test.csv 不存在，Python 会自动创建文件(不会自动创建文件夹)。如果文件已经存在，Python 会 用新的数据覆盖 test.csv 文件。


运行完成后，你会看到一个 CSV 文件:

      number,number plus 2,number times 2
      0,2,0
      1,3,2
      2,4,4 ...

网络数据采集的一个常用功能就是获取 HTML 表格并写入 CSV 文件。维基百科的文本编 辑 器 对 比 词 条(https://en.wikipedia.org/wiki/Comparison_of_text_editors) 中 用 了 许 多 复 杂 的 HTML 表格，用到了颜色、链接、排序，以及其他在写入 CSV 文件之前需要忽略的 HTML 元素。用 BeautifulSoup 和 get_text() 函数，你可以用十几行代码完成这件事:

         import csv
         from urllib.request import urlopen 
         from bs4 import BeautifulSoup
         html = urlopen("http://en.wikipedia.org/wiki/Comparison_of_text_editors") 
         bsObj = BeautifulSoup(html)
         # 主对比表格是当前页面上的第一个表格
         table = bsObj.findAll("table",{"class":"wikitable"})[0]
         rows = table.findAll("tr")
         
         csvFile = open("../files/editors.csv", 'wt', newline=", encoding='utf-8')
         writer = csv.writer(csvFile)
         try:
            for row in rows:
            csvRow = []
            for cell in row.findAll(['td', 'th']):
                     csvRow.append(cell.get_text())
                     writer.writerow(csvRow) 
         finally:
            csvFile.close()


         实际工作中写此程序之前的注意事项
      如果你有很多 HTML 表格，且每个都要转换成 CSV 文件，或者许多 HTML 表格都要 汇总到一个 CSV 文件，那么把这个程序整合到爬虫里以解决问题非常好。但是，如果 你只需要做一次这种事情，那么更好的办法就是:复制粘贴。选择 HTML 表格内容然 后粘贴到 Excel 文件里，可以另存为 CSV 格式，不需要写代码就能搞定!
      
这个程序会在程序上一层目录的 files 文件夹里生成一个 CSV 文件 ../files/editors.csv——把 这个程序分享给那些不熟悉 MySQL 的朋友们吧!

# 5.3 MySQL
MySQL(官方发音是“My es-kew-el”，但很多人都说成“My Sequel”)是目前最受欢迎 的开源关系型数据库管理系统。一个开源项目具有如此之竞争力实在是令人意外，它的流 行程度正在不断地接近另外两个闭源的商业数据库系统:微软的 SQL Server 和甲骨文的 Oracle 数据库(MySQL 在 2010 年被甲骨文收购)。

它的流程程度实在是名符其实。对大多数应用来说，MySQL 都是不二选择。它是一种 非常灵活、稳定、功能齐全的 DBMS，许多顶级的网站都在用它:YouTube1、Twitter2 和 Facebook3 等


因为它受众广泛，免费，开箱即用，所以它也是网络数据采集项目中常用的数据库，我们 将在本书后面的示例中使用它。

         “关系型”数据库 ? “关系型数据”就是有关联的数据。就是这么简单!
         开个玩笑!当计算机科学家说起关系型数据时，他们指的是那些并非孤立的数据—— 它们的属性与其他的数据是有关联的。例如，“用户 A 在学校 B 上学”，这里用户 A 在数据库的“用户”表中，而学校 B 是在数据库的“学校”表中。
         在本章后面的内容里，我们将介绍数据关系的不同类型，以及如何有效地把数据存储 到 MySQL(或其他关系型数据库)里。
         
# 5.3.1 安装MySQL
如果你第一次接触 MySQL，安装数据库听着可能有点儿吓人(如果你是老手，可以跳过 这部分内容)。其实，安装方法和安装其他软件一样简单。归根到底，MySQL 就是由一系 列数据文件构成的，储存在你的远端服务器或本地的电脑上，里面包含了数据库存储的所 有信息。MySQL 软件层提供了一种与数据交互的便捷操作方法。例如，下面的命令把用 户表 users 中名字为“Ryan”的用户找出来:         

      SELECT * FROM users WHERE firstname = "Ryan"
      
如果你用 Ubuntu(或其他 Debain 分支系统)，安装 MySQL 很简单:

       $sudo apt-get install mysql-server
       

只要稍微留意一下安装过程，看看电脑是不是可以满足安装的内存需求，然后在安装提示 的地方为 root 用户设置新密码就可以了


Mac OS X 和 Windows 系统的安装过程有点儿复杂。如果你没有甲骨文账户，下载 MySQL 安装包之前需要先注册一下。

如 果 你 用 Mac OS X 系 统， 请 先 下 载 对 应 的 安 装 包(http://dev.mysql.com/downloads/ mysql/)。

选择 .dmg 安装包，登入网站或者创建一个账户，开始下载文件。下载完成后打开安装包， 你会看到一个简单的安装向导(图 5-1)。

![](https://github.com/linbearababy/phthon-deep-/blob/master/catagory/python%20%E7%88%AC%E8%99%AB/pictures/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-09%2022.52.01.png)

使用默认安装步骤就可以，本书后面使用 MySQL 都是假设你用的默认安装步骤。

如果觉得下载安装包再执行安装工具太无聊，也可以用 Mac OS X 的包管理器 Homebrew (http://brew.sh/)安装。当 Homebrew 安装好以后，用下面的命令安装 MySQL:

       $brew install mysql
       
Homebrew 是一个伟大的开源工具，与 Python 包完美结合。其实，本书使用的大多数 Python 第三方库都可以用 Homebrew 安装。如果你还没用过，强烈推荐你试一下!

Mac OS X 的 MySQL 安装好之后，你可以用下面的命令启动 MySQL 服务器: 

        $cd /usr/local/mysql
        $sudo ./bin/mysqld_safe
        
在 Windows 系统上，安装和运行 MySQL 更复杂一些，但是有个方便的安装工具(http:// dev.mysql.com/downloads/windows/installer/)可以简化这个过程(图 5-2)。
