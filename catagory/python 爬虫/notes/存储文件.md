虽然在命令行里显示运行结果很有意思，但是随着数据不断增多，并且需要进行数据分析 时，将数据打印到命令行就不是办法了。为了可以远程使用大部分网络爬虫，你还需要把 采集到的数据存储起来。

本章将介绍三种主要的数据管理方法，对绝大多数应用都适用。如果你准备创建一个网站 的后端服务或者创建自己的 API，那么可能都需要让爬虫把数据写入数据库。如果你需要 一个快速简单的方法收集网上的文档，然后存到你的硬盘里，那么可能需要创建一个文件 流(file stream)来实现。如果还要为偶然事件提个醒儿，或者每天定时收集当天累计的数 据，就给自己发一封邮件吧!

抛开与网络数据采集的关系，大数据存储和与数据交互的能力，在新式的程序开发中也已 经是重中之重了。这一章的内容其实是实现第二部分许多示例的基础。如果你对自动数据 存储相关的知识不太了解，我非常希望你至少能浏览一下。

# 5.1 媒体文件

存储媒体文件有两种主要的方式:只获取文件 URL 链接，或者直接把源文件下载下来。
你可以通过媒体文件所在的 URL 链接直接引用它。这样做的优点如下:
   
      • 爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。
      • 可以节省很多存储空间，因为只需要存储 URL 链接就可以。
      • 存储 URL 的代码更容易写，也不需要实现文件下载代码。 • 不下载文件能够降低目标主机服务器的负载。
      
不过这么做也有一些缺点。
      
      • 这些内嵌在你的网站或应用中的外站 URL 链接被称为盗链(hotlinking)，使用盗链可 能会让你麻烦不断，每个网站都会实施防盗链措施。
      • 因为你的链接文件在别人的服务器上，所以你的应用就要跟着别人的节奏运行了。
      • 盗链是很容易改变的。如果你把盗链图片放在博客上，要是被对方服务器发现，很可能 被恶搞。如果你把 URL 链接存起来准备以后再用，可能用的时候链接已经失效了，或
       者是变成了完全无关的内容。
      • 现实中的网络浏览器不仅可以请求 HTML 页面并切换页面，它们也会下载访问页面上
       所有的资源。下载文件会让你的爬虫看起来更像是人在浏览网站，这样做反而有好处。

如果你还在犹豫究竟是存储文件，还是只存储文件的 URL 链接，可以想想这些文件是要 多次使用，还是放进数据库之后就只是等着“落灰”，再也不会被打开。如果答案是后者， 那么最好还是只存储这些文件的 URL 吧。如果答案是前者，那么就继续往下看!

在 Python 3.x 版本中，urllib.request.urlretrieve 可以根据文件的 URL 下载文件:

    from urllib.request import urlretrieve 
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup
    
    html = urlopen("http://www.pythonscraping.com")
    bsObj = BeautifulSoup(html)
    imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
    urlretrieve (imageLocation, "logo.jpg")
    
  这段程序从 http://pythonscraping.com 下载 logo 图片，然后在程序运行的文件夹里保存为 logo.jpg 文件。
  
  
