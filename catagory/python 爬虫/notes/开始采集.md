到目前为止，本书的例子都只是处理单个静态页面，只能算是人为简化的例子（使用作者的网站页面)。从本章开始，我们会看到一些现实问题，需要用爬虫遍历多个页面甚至多 个网站。
之所以叫网络爬虫(Web crawler)是因为它们可以沿着网络爬行。它们的本质就是一种递 归方式。为了找到 URL 链接，它们必须首先获取网页内容，检查这个页面的内容，再寻 找另一个 URL，然后获取 URL 对应的网页内容，不断循环这一过程。
不过要注意的是:你可以这样重复采集网页，但并不意味着你一直都应该这么做。当你需 要的所有数据都在一个页面上时，前面例子中的爬虫就足以解决问题了。使用网络爬虫的 时候，你必须非常谨慎地考虑需要消耗多少网络流量，还要尽力思考能不能让采集目标的 服务器负载更低一些。

    HTML 标签：
    http://www.w3school.com.cn/tags
    
# 3.1 遍历单个域名
即使你没听说过“维基百科六度分隔理论”，也很可能听过“凯文· 贝肯(Kevin Bacon) 的六度分隔值游戏”。在这两个游戏中，都是把两个不相干的主题(维基百科里是用词条 之间的连接，凯文 · 贝肯的六度分隔值游戏是用出现在同一部电影中的演员来连接)用一 个总数不超过六条的主题连接起来(包括原来的两个主题)。
比如，埃里克 · 艾德尔和布兰登 · 弗雷泽都出现在电影《骑警杜德雷》里，布兰登 · 弗 雷泽又和凯文·贝肯都出现在电影《我呼吸的空气》里。1 因此，根据这两个条件，从埃
里克· 艾德尔到凯文·贝肯的链条主题长度只有 3。
我们将在本节创建一个项目来实现“维基百科六度分隔理论”的查找方法。也就是说，我们 要实现从埃里克· 艾德尔的词条页面(https://en.wikipedia.org/wiki/Eric_Idle)开始，经过最 少的链接点击次数找到凯文· 贝肯的词条页面(https://en.wikipedia.org/wiki/Kevin_Bacon)。

# 这么做对维基百科的服务器负载有多大影响 ?
根据维基媒体基金会(维基百科归属的组织)的统计，网站每秒钟会收到大约 2500
次点击，其中超过 99% 的点击都是指向维基百科域名(详情请见“维基媒体统计图” (Wikimedia in Figures)里的“流量数据”(Traffic Volume)部分内容，https://meta. wikimedia.org/wiki/Wikimedia_in_figures_-_Wikipedia#Traffic_volume)。因为网站流量 很大，所以你的网络爬虫不可能对维基百科的负载有显著影响。不过，如果你频繁地 运行本书的代码，或者自己在做项目采集维基百科的词条，那么希望你能够向维基媒 体基金会提供一点捐赠(https://wikimediafoundation.org/wiki/Ways_to_Give)——即 使是很少的钱来补偿你占用的服务器资源，算是帮助维基百科这个教育资源供其他人
使用。

你应该已经知道如何写一段获取维基百科网站的任何页面并提取页面链接的 Python 代码了: 
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon") 
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll("a"):
      if 'href' in link.attrs: 
          print(link.attrs['href'])
    
    
    ——————————————————————————————
    <a> 标签定义超链接，用于从一张页面链接到另一张页面。

    <a> 元素最重要的属性是 href 属性，它指示链接的目标。
    
如果你观察生成的一列链接，就会看到你想要的所有词条链接都在里面:“Apollo 13” “Philadelphia”和“Primetime Emmy Award”，等等。但是，也有一些我们不需要的链接:
       
       //wikimediafoundation.org/wiki/Privacy_policy
       //en.wikipedia.org/wiki/Wikipedia:Contact_us
其实维基百科的每个页面都充满了侧边栏、页眉、页脚链接，以及连接到分类页面、对话 页面和其他不包含词条的页面的链接:
       
       /wiki/Category:Articles_with_unsourced_statements_from_April_2014
       /wiki/Talk:Kevin_Bacon

最近我有个朋友在做一个类似维基百科采集这样的项目，他说为了判断维基百科的内链是 否链接到一个词条，他写了一个很大的过滤函数，超过 100 行代码。不幸的是，可能在项 目启动的时候，他没有花时间去比较“词条链接”和“其他链接”的差异，也可能他后来 发现了那个技巧。如果你仔细观察那些指向词条页面(不是指向其他内容页面)的链接，
会发现它们都有三个共同点:

    • 它们都在 id 是 bodyContent 的 div 标签里
    • URL 链接不包含分号
    • URL 链接都以 /wiki/ 开头
    我们可以利用这些规则稍微调整一下代码来获取词条链接:
    
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup 
    import re
    html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon")
    bsObj = BeautifulSoup(html)
    for link in bsObj.find("div",       {"id":"bodyContent"}).findAll("a",href=re.compile("^(/wiki/)((?!:).)*$")): 
      if 'href' in link.attrs:
        print(link.attrs['href'])
        
如果你运行代码，就会看到维基百科上凯文·贝肯词条里所有指向其他词条的链接。
当然，写程序来找出这个静态的维基百科词条里所有的词条链接很有趣，不过没什么实际 用处。我们需要让这段程序更像下面的形式。

• 一个函数 getLinks，可以用维基百科词条 /wiki/< 词条名称 > 形式的 URL 链接作为参数， 然后以同样的形式返回一个列表，里面包含所有的词条 URL 链接。

• 一个主函数，以某个起始词条为参数调用 getLinks，再从返回的 URL 列表里随机选择 一个词条链接，再调用 getLinks，直到我们主动停止，或者在新的页面上没有词条链接 了，程序才停止运行。

完整的代码如下所示:

    from urllib.request import urlopen 
    from bs4 import BeautifulSoup 
    import datetime
    import random
    import re
    random.seed(datetime.datetime.now()) 
    def getLinks(articleUrl):
      html = urlopen("http://en.wikipedia.org"+articleUrl)
      bsObj = BeautifulSoup(html)
       return bsObj.find("div", {"id":"bodyContent"}).findAll("a",
                              href=re.compile("^(/wiki/)((?!:).)*$"))
    links = getLinks("/wiki/Kevin_Bacon")
    while len(links) > 0:
      newArticle = links[random.randint(0, len(links)-1)].attrs["href"] 
      print(newArticle)
      links = getLinks(newArticle)
导入需要的 Python 库之后，程序首先做的是用系统当前时间生成一个随机数生成器。这样 可以保证在每次程序运行的时候，维基百科词条的选择都是一个全新的随机路径。

        伪随机数和随机数种子

        在前面的示例中，为了能够连续地随机遍历维基百科，我用 Python 的随机数生成器来 随机选择每一页上的一个词条链接。但是，用随机数的时候需要格外小心。

        虽然计算机很擅长做精确计算，但是它们处理随机事件时非常不靠谱。因此，随机数 是一个难题。大多数随机数算法都努力创造一种呈均匀分布且难以预测的数据序列， 但是在算法初始化阶段都需要提供随机数“种子”(random seed)。而完全相同的种子 每次将产生同样的“随机”数序列，因此我用系统时间作为随机数序列生成的起点。 这样做会让程序运行的时候更具有随机性。

        其实，Python 的伪随机数(pseudorandom number)生成器用的是梅森旋转(Mersenne Twister)算法(https://en.wikipedia.org/wiki/Mersenne_Twister)，它产生的随机数很难 预测且呈均匀分布，就是有点儿耗费 CPU 资源。真正好的随机数可不便宜!

然后，我们定义 getLinks 函数，其参数是维基百科词条页面中 /wiki/< 词条名称 > 形式的 URL 链接，前面加上维基百科的域名，http://en.wikipedia.org，再用域名中的网页获得 一个 BeautifulSoup 对象。之后用前面介绍过的参数抽取一列词条链接所在的标签 a 并返回 它们。
程序的主函数首先把起始页面 https://en.wikipedia.org/wiki/Kevin_Bacon 里的词条链接列表 (links 变量)设置成链接列表。然后用一个循环，从页面中随机找一个词条链接标签并抽 取 href 属性，打印这个页面链接，再把这个链接传入 getLinks 函数，重新获取新的链接
列表。
当然，这里只是简单地构建一个从一个页面到另一个页面的爬虫，要解决“维基百科六度 分隔理论”问题还有一点儿工作得做。我们还应该存储 URL 链接数据并分析数据。关于 这个问题后续的解决办法，请参考第 5 章内容。

异常处理

虽然为了方便起见，我们在这些示例中忽略了大多数异常处理过程，但是要 注意问题随时可能发生。例如，维基百科改变了 bodyContent 标签的名称怎 么办呢?(提示:那时代码就会崩溃。)

因此，这些脚本作为容易演示的示例也许可以运行得很不错，但是要真正成 为自动化产品代码，还需要增加更多的异常处理。关于异常处理的更多信 息，请参考第 1 章的相关内容


# 3.2 采集整个网站
在上一节内容里，我们实现了在一个网站上随机地从一个链接跳到另一个链接。但是，如 果你需要系统地把整个网站按目录分类，或者要搜索网站上的每一个页面，怎么办?那就 得采集整个网站，那是一种非常耗费内存资源的过程，尤其是处理大型网站时，最合适的 工具就是用一个数据库来储存采集的资源。但是，我们可以掌握这类工具的行为，并不需 要通过大规模地运行它们。要了解更多关于数据库使用的相关知识，请参考第 5 章。

                                    深网和暗网
            你可能听说过深网(deep Web)、暗网(dark Web)或隐藏网络(hidden Web)之类的术语，尤其是在最近的媒体中。它们是什么意思呢?
            深网是网络的一部分，与浅网(surface Web)对立。浅网是互联网上搜索引擎可以抓 到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不 能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因 为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。
            暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有 的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个 信息交换的安全隧道。这类暗网页面也是可以采集的，就像你采集其他网站一样，不 过这些内容超出了本书的范围。
            和暗网不同，深网是相对容易采集的。实际上，本书的很多工具都是在教你如何采集 那些 Google 爬虫机器人不能获取的深网信息。
            
那么，什么时候采集整个网站是有用的，而什么时候采集整个网站又是有害无益的呢?遍 历整个网站的网络数据采集有许多好处。

• 生成网站地图 几年前，我曾经遇到过一个问题:一个重要的客户想对一个网站的重新设计方案进行效 果评估，但是不想让我们公司进入他们的网站内容管理系统(CMS)，也没有一个公开 可用的网站地图。我就用爬虫采集了整个网站，收集了所有的链接，再把所有的页面整 理成他们网站实际的形式。这让我很快找出了网站上以前不曾留意的部分，并准确地计 算出需要重新设计多少网页，以及可能需要移动多少内容。

• 收集数据 我的另一个客户为了创建一个专业垂直领域的搜索平台，想收集一些文章(故事、博 文、新闻等)。虽然这些网站采集并不费劲，但是它们需要爬虫有足够的深度(我们有 意收集数据的网站不多)。于是我就创建了一个爬虫递归地遍历每个网站，只收集那些 网站页面上的数据。

一个常用的费时的网站采集方法就是从顶级页面开始(比如主页)，然后搜索页面上的所 有链接，形成列表。再去采集这些链接的每一个页面，然后把在每个页面上找到的链接形 成新的列表，重复执行下一轮采集。

很明显，这是一个复杂度增长很快的情形。假如每个页面有 10 个链接，网站上有 5 个页 面深度(一个中等规模网站的主流深度)，那么如果你要采集整个网站，一共得采集的网 页数量就是 105，即 100 000 个页面。不过，虽然“5 个页面深度，每页 10 个链接”是网 站的主流配置，但其实很少有网站真的有 100 000 甚至更多的页面。这是因为很大一部分 内链都是重复的。

为了避免一个页面被采集两次，链接去重是非常重要的。在代码运行时，把已发现的所有 链接都放到一起，并保存在方便查询的列表里(下文示例指 Python 的集合 set 类型)。只 有“新”链接才会被采集，之后再从页面中搜索其他链接:

        from urllib.request import urlopen 
        from bs4 import BeautifulSoup 
        import re

        pages = set()
        def getLinks(pageUrl):
            global pages
            html = urlopen("http://en.wikipedia.org"+pageUrl)
            bsObj = BeautifulSoup(html)
            for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")):
                if 'href' in link.attrs:
                       if link.attrs['href'] not in pages:
                       # 我们遇到了新页面
                       newPage = link.attrs['href']
                       print(newPage)
                       pages.add(newPage)
                       getLinks(newPage)
        getLinks("")

为了全面地展示这个网络数据采集示例是如何工作的，我降低了在前面例子里使用的“只 寻找内链”的标准。不再限制爬虫采集的页面范围，只要遇到页面就查找所有以 /wiki/ 开 头的链接，也不考虑链接是不是包含分号。(提示:词条链接不包含分号，而文档上传页 面、讨论页面之类的页面 URL 链接都包含分号。)

一开始，用 getLinks 处理一个空 URL，其实是维基百科的主页，因为在函数里空 URL 就 是 http://en.wikipedia.org。然后，遍历首页上每个链接，并检查是否已经在全局变量 集合 pages 里面了(已经采集的页面集合)。如果不在，就打印到屏幕上，并把链接加入 pages 集合，再用 getLinks 递归地处理这个链接。

                        关于递归的警告
    这个警告在软件开发书籍里很少提到，但是我觉得你应该注意:如果递归运 行的次数非常多，前面的递归程序就很可能崩溃。
    Python 默认的递归限制(程序递归地自我调用次数)是 1000 次。因为维基 百科的网络链接浩如烟海，所以这个程序达到递归限制后就会停止，除非你 设置一个较大的递归计数器，或用其他手段不让它停止。
    对于那些链接深度少于 1000 的“普通”网站，这个方法通常可以正常运行， 一些奇怪的异常除外。例如，我曾经遇到过一个网站，有一个在生成博文内 链的规则。这个规则是“当前页面把 /blog/title_of_blog.php 加到它后面，作 为本页面的 URL 链接”。
    问题是它们可能会把 /blog/title_of_blog.php 加到一个已经有 /blog/ 的 URL 上 面了。因此，网站就多了一个 /blog/。最后，我的爬虫找到了这样的 URL 链 接:/blog/blog/blog/blog.../blog/title_of_blog.php。
    后来，我增加了一些条件，对可能导致无限循环的部分进行检查，确保那些 URL 不是这么荒谬。但是，如果你不去检查这些问题，爬虫很快就会崩溃。
    
# 收集整个网站数据
