到目前为止，本书的例子都只是处理单个静态页面，只能算是人为简化的例子（使用作者的网站页面)。从本章开始，我们会看到一些现实问题，需要用爬虫遍历多个页面甚至多 个网站。
之所以叫网络爬虫(Web crawler)是因为它们可以沿着网络爬行。它们的本质就是一种递 归方式。为了找到 URL 链接，它们必须首先获取网页内容，检查这个页面的内容，再寻 找另一个 URL，然后获取 URL 对应的网页内容，不断循环这一过程。
不过要注意的是:你可以这样重复采集网页，但并不意味着你一直都应该这么做。当你需 要的所有数据都在一个页面上时，前面例子中的爬虫就足以解决问题了。使用网络爬虫的 时候，你必须非常谨慎地考虑需要消耗多少网络流量，还要尽力思考能不能让采集目标的 服务器负载更低一些。

# 3.1 遍历单个域名
即使你没听说过“维基百科六度分隔理论”，也很可能听过“凯文· 贝肯(Kevin Bacon) 的六度分隔值游戏”。在这两个游戏中，都是把两个不相干的主题(维基百科里是用词条 之间的连接，凯文 · 贝肯的六度分隔值游戏是用出现在同一部电影中的演员来连接)用一 个总数不超过六条的主题连接起来(包括原来的两个主题)。
比如，埃里克 · 艾德尔和布兰登 · 弗雷泽都出现在电影《骑警杜德雷》里，布兰登 · 弗 雷泽又和凯文·贝肯都出现在电影《我呼吸的空气》里。1 因此，根据这两个条件，从埃
里克· 艾德尔到凯文·贝肯的链条主题长度只有 3。
我们将在本节创建一个项目来实现“维基百科六度分隔理论”的查找方法。也就是说，我们 要实现从埃里克· 艾德尔的词条页面(https://en.wikipedia.org/wiki/Eric_Idle)开始，经过最 少的链接点击次数找到凯文· 贝肯的词条页面(https://en.wikipedia.org/wiki/Kevin_Bacon)。

# 这么做对维基百科的服务器负载有多大影响 ?
根据维基媒体基金会(维基百科归属的组织)的统计，网站每秒钟会收到大约 2500
次点击，其中超过 99% 的点击都是指向维基百科域名(详情请见“维基媒体统计图” (Wikimedia in Figures)里的“流量数据”(Traffic Volume)部分内容，https://meta. wikimedia.org/wiki/Wikimedia_in_figures_-_Wikipedia#Traffic_volume)。因为网站流量 很大，所以你的网络爬虫不可能对维基百科的负载有显著影响。不过，如果你频繁地 运行本书的代码，或者自己在做项目采集维基百科的词条，那么希望你能够向维基媒 体基金会提供一点捐赠(https://wikimediafoundation.org/wiki/Ways_to_Give)——即 使是很少的钱来补偿你占用的服务器资源，算是帮助维基百科这个教育资源供其他人
使用。

你应该已经知道如何写一段获取维基百科网站的任何页面并提取页面链接的 Python 代码了: 
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon") 
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll("a"):
      if 'href' in link.attrs: 
          print(link.attrs['href'])
    
    
    ——————————————————————————————
    <a> 标签定义超链接，用于从一张页面链接到另一张页面。

    <a> 元素最重要的属性是 href 属性，它指示链接的目标。
    
如果你观察生成的一列链接，就会看到你想要的所有词条链接都在里面:“Apollo 13” “Philadelphia”和“Primetime Emmy Award”，等等。但是，也有一些我们不需要的链接:
       
       //wikimediafoundation.org/wiki/Privacy_policy
       //en.wikipedia.org/wiki/Wikipedia:Contact_us
其实维基百科的每个页面都充满了侧边栏、页眉、页脚链接，以及连接到分类页面、对话 页面和其他不包含词条的页面的链接:
       
       /wiki/Category:Articles_with_unsourced_statements_from_April_2014
       /wiki/Talk:Kevin_Bacon

最近我有个朋友在做一个类似维基百科采集这样的项目，他说为了判断维基百科的内链是 否链接到一个词条，他写了一个很大的过滤函数，超过 100 行代码。不幸的是，可能在项 目启动的时候，他没有花时间去比较“词条链接”和“其他链接”的差异，也可能他后来 发现了那个技巧。如果你仔细观察那些指向词条页面(不是指向其他内容页面)的链接，
会发现它们都有三个共同点:

    • 它们都在 id 是 bodyContent 的 div 标签里
    • URL 链接不包含分号
    • URL 链接都以 /wiki/ 开头
    我们可以利用这些规则稍微调整一下代码来获取词条链接:
    
    from urllib.request import urlopen 
    from bs4 import BeautifulSoup 
    import re
    html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon")
    bsObj = BeautifulSoup(html)
    for link in bsObj.find("div",       {"id":"bodyContent"}).findAll("a",href=re.compile("^(/wiki/)((?!:).)*$")): 
      if 'href' in link.attrs:
        print(link.attrs['href'])
        
如果你运行代码，就会看到维基百科上凯文·贝肯词条里所有指向其他词条的链接。
当然，写程序来找出这个静态的维基百科词条里所有的词条链接很有趣，不过没什么实际 用处。我们需要让这段程序更像下面的形式。

• 一个函数 getLinks，可以用维基百科词条 /wiki/< 词条名称 > 形式的 URL 链接作为参数， 然后以同样的形式返回一个列表，里面包含所有的词条 URL 链接。

• 一个主函数，以某个起始词条为参数调用 getLinks，再从返回的 URL 列表里随机选择 一个词条链接，再调用 getLinks，直到我们主动停止，或者在新的页面上没有词条链接 了，程序才停止运行。

完整的代码如下所示:

    from urllib.request import urlopen 
    from bs4 import BeautifulSoup 
    import datetime
    import random
    import re
    random.seed(datetime.datetime.now()) 
    def getLinks(articleUrl):
      html = urlopen("http://en.wikipedia.org"+articleUrl)
      bsObj = BeautifulSoup(html)
       return bsObj.find("div", {"id":"bodyContent"}).findAll("a",
                              href=re.compile("^(/wiki/)((?!:).)*$"))
    links = getLinks("/wiki/Kevin_Bacon")
    while len(links) > 0:
      newArticle = links[random.randint(0, len(links)-1)].attrs["href"] 
      print(newArticle)
      links = getLinks(newArticle)
导入需要的 Python 库之后，程序首先做的是用系统当前时间生成一个随机数生成器。这样 可以保证在每次程序运行的时候，维基百科词条的选择都是一个全新的随机路径。

        伪随机数和随机数种子

        在前面的示例中，为了能够连续地随机遍历维基百科，我用 Python 的随机数生成器来 随机选择每一页上的一个词条链接。但是，用随机数的时候需要格外小心。

        虽然计算机很擅长做精确计算，但是它们处理随机事件时非常不靠谱。因此，随机数 是一个难题。大多数随机数算法都努力创造一种呈均匀分布且难以预测的数据序列， 但是在算法初始化阶段都需要提供随机数“种子”(random seed)。而完全相同的种子 每次将产生同样的“随机”数序列，因此我用系统时间作为随机数序列生成的起点。 这样做会让程序运行的时候更具有随机性。

        其实，Python 的伪随机数(pseudorandom number)生成器用的是梅森旋转(Mersenne Twister)算法(https://en.wikipedia.org/wiki/Mersenne_Twister)，它产生的随机数很难 预测且呈均匀分布，就是有点儿耗费 CPU 资源。真正好的随机数可不便宜!

然后，我们定义 getLinks 函数，其参数是维基百科词条页面中 /wiki/< 词条名称 > 形式的 URL 链接，前面加上维基百科的域名，http://en.wikipedia.org，再用域名中的网页获得 一个 BeautifulSoup 对象。之后用前面介绍过的参数抽取一列词条链接所在的标签 a 并返回 它们。
程序的主函数首先把起始页面 https://en.wikipedia.org/wiki/Kevin_Bacon 里的词条链接列表 (links 变量)设置成链接列表。然后用一个循环，从页面中随机找一个词条链接标签并抽 取 href 属性，打印这个页面链接，再把这个链接传入 getLinks 函数，重新获取新的链接
列表。
当然，这里只是简单地构建一个从一个页面到另一个页面的爬虫，要解决“维基百科六度 分隔理论”问题还有一点儿工作得做。我们还应该存储 URL 链接数据并分析数据。关于 这个问题后续的解决办法，请参考第 5 章内容。

异常处理

虽然为了方便起见，我们在这些示例中忽略了大多数异常处理过程，但是要 注意问题随时可能发生。例如，维基百科改变了 bodyContent 标签的名称怎 么办呢?(提示:那时代码就会崩溃。)

因此，这些脚本作为容易演示的示例也许可以运行得很不错，但是要真正成 为自动化产品代码，还需要增加更多的异常处理。关于异常处理的更多信 息，请参考第 1 章的相关内容
